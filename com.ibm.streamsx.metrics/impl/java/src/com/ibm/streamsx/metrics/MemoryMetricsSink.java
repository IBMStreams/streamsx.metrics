/* Generated by Streams Studio: December 4, 2016 at 12:53:26 AM EST */
package com.ibm.streamsx.metrics;


import java.net.InetAddress;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.concurrent.TimeUnit;

import org.apache.log4j.Logger;
import org.elasticsearch.action.admin.indices.create.CreateIndexRequest;
import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
import org.elasticsearch.action.index.IndexResponse;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.action.search.SearchType;
import org.elasticsearch.client.Requests;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.transport.InetSocketTransportAddress;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.common.xcontent.XContentFactory;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.SearchHit;
import org.elasticsearch.transport.client.PreBuiltTransportClient;

import com.ibm.streams.operator.AbstractOperator;
import com.ibm.streams.operator.OperatorContext;
import com.ibm.streams.operator.OutputTuple;
import com.ibm.streams.operator.StreamingData.Punctuation;
import com.ibm.streams.operator.StreamingInput;
import com.ibm.streams.operator.StreamingOutput;
import com.ibm.streams.operator.Tuple;
import com.ibm.streams.operator.model.InputPortSet;
import com.ibm.streams.operator.model.InputPortSet.WindowMode;
import com.ibm.streams.operator.model.InputPortSet.WindowPunctuationInputMode;
import com.ibm.streams.operator.model.OutputPortSet.WindowPunctuationOutputMode;
import com.ibm.streams.operator.model.InputPorts;
import com.ibm.streams.operator.model.Libraries;
import com.ibm.streams.operator.model.OutputPortSet;
import com.ibm.streams.operator.model.OutputPorts;
import com.ibm.streams.operator.model.Parameter;
import com.ibm.streams.operator.model.PrimitiveOperator;

import org.influxdb.InfluxDB;
import org.influxdb.InfluxDB.ConsistencyLevel;
import org.influxdb.InfluxDBFactory;
import org.influxdb.dto.BatchPoints;
import org.influxdb.dto.Point;

@PrimitiveOperator(name="MemoryMetricsSink", namespace="com.ibm.streamsx.metrics",
description="Java Operator MemoryMetricsSink")
@InputPorts({@InputPortSet(description="Port that ingests tuples", cardinality=1, optional=false, windowingMode=WindowMode.NonWindowed, windowPunctuationInputMode=WindowPunctuationInputMode.Oblivious), @InputPortSet(description="Optional input ports", optional=true, windowingMode=WindowMode.NonWindowed, windowPunctuationInputMode=WindowPunctuationInputMode.Oblivious)})
@OutputPorts({
	@OutputPortSet(
			cardinality=1,
			optional=true,
			windowPunctuationOutputMode=WindowPunctuationOutputMode.Generating,
			description=MemoryMetricsSink.DESC_OUTPUT_PORT
			)
})
@Libraries({
	// As described here:
	// http://www.ibm.com/support/knowledgecenter/en/SSCRJU_4.2.0/com.ibm.streams.dev.doc/doc/jmxapi-start.html
	// Environment variables (@...@) are evaluated during compile-time and must
	// be identical in the run-time environment.
	"@STREAMS_INSTALL@/lib/com.ibm.streams.management.jmxmp.jar",
	"@STREAMS_INSTALL@/lib/com.ibm.streams.management.mx.jar",
	"@STREAMS_INSTALL@/ext/lib/jmxremote_optional.jar",
	"opt/downloaded/*"
	})
public class MemoryMetricsSink extends AbstractOperator {
	
	// Parameter definition.
	private static final String ROLLING_AVG_INTERVAL = 
			"Specifies the interval to calculate rolling average at (in seconds).";
	
	protected static final String DESC_OUTPUT_PORT = 
			"Outputs alert to email when memory leak is detected.";

	int rollingAverageInterval = 0;
	
	@Parameter(
			optional=false,
			description=MemoryMetricsSink.ROLLING_AVG_INTERVAL
			)
	public void set_rolling_avg_interval(int interval) {
		rollingAverageInterval = interval;
	}
	
	// InfluxDB variables.
	InfluxDB influxDB = null;
	String dbName = null;
	BatchPoints batchPoints = null;
	String jobName = null;
	
	// ElasticSearch variables.
	TransportClient client = null;
	XContentBuilder builder = null;
	IndexResponse response = null;
	
	DateFormat df = null;
	boolean alerted = false;
	boolean once = false;
	private static Logger _trace = Logger.getLogger(MemoryMetricsSink.class.getName());
	
	@SuppressWarnings("resource")
	@Override
	public synchronized void initialize(OperatorContext context)
			throws Exception {
		super.initialize(context);
        Logger.getLogger(this.getClass()).trace("Operator " + context.getName() + " initializing in PE: " + context.getPE().getPEId() + " in Job: " + context.getPE().getJobId() );
        
        // Connect to InfluxDB server.
        influxDB = InfluxDBFactory.connect("http://localhost:8086", "admin", "admin");
    	dbName = "streamsDb";
    	influxDB.createDatabase(dbName);
        
    	// Initialize object to contain batch of points.
    	batchPoints = BatchPoints
	    	.database(dbName)
    		.tag("async", "true")
            .retentionPolicy("autogen")
            .consistency(ConsistencyLevel.ALL)
            .build();
    	
    	// Connect to ElasticSearch server.
		client = new PreBuiltTransportClient(Settings.EMPTY).addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300));
		//CreateIndexResponse createResponse = client.admin().indices().create(Requests.createIndexRequest("streamsdb")).actionGet();
		
		// Date format for ElasticSearch.
		df = new SimpleDateFormat("yyyy'-'MM'-'dd'T'HH':'mm':'ss.SSSZZ");
	}

    @Override
    public synchronized void allPortsReady() throws Exception {
        OperatorContext context = getOperatorContext();
        Logger.getLogger(this.getClass()).trace("Operator " + context.getName() + " all ports are ready in PE: " + context.getPE().getPEId() + " in Job: " + context.getPE().getJobId() );
    }

    @Override
    public void process(StreamingInput<Tuple> stream, Tuple tuple)
            throws Exception {
    	
    	int rollingAverage = calculateRollingAvg(tuple.getString("metricName"), tuple.getLong("peId"));
    	
    	_trace.error("tuples:[domain:" + tuple.getString("domainName") 
    	                  + ",instance:" + tuple.getString("instanceName") 
    	                  + ",peId:" + tuple.getLong("peId")
    	                  + "," + tuple.getString("metricName") + ":"
    	                  + String.valueOf(tuple.getLong("metricValue")) + "]");
    	
    	// Create InfluxDB point to output.
        Point point = Point.measurement("memoryMetrics")
            .time(tuple.getLong("lastTimeRetrieved"), TimeUnit.MILLISECONDS)
            .addField(tuple.getString("metricName"), tuple.getLong("metricValue"))
            .addField(tuple.getString("metricName") + "RollingAvg", rollingAverage)
            .addField("domainName", tuple.getString("domainName"))
            .addField("instanceName", tuple.getString("instanceName"))
            .addField("peId", tuple.getLong("peId"))
            .build();
        
        // Store in batch until window marker is received.
        batchPoints.point(point);
        
        // Create ElasticSearch JSON to output.
        builder = XContentFactory.jsonBuilder()
        		.startObject()
	        		.field("lastTimeRetrieved", df.format(new Date((tuple.getLong("lastTimeRetrieved")))))
    				.field(tuple.getString("metricName"), 
    						Integer.parseInt(tuple.getString("metricValue")))
    				.field(tuple.getString("metricName") + "RollingAvg", rollingAverage)
    	            .field("domainName", tuple.getString("domainName"))
    	            .field("instanceName", tuple.getString("instanceName"))
    	            .field("peId", tuple.getLong("peId"))
        		.endObject();
        
        if(memoryLeaking(tuple.getLong("peId"))) {
        	// Alert if memory is leaking.
    		if(!alerted) {
    			StreamingOutput<OutputTuple> output = getOutput(0);
    			OutputTuple alertTuple = output.newTuple();
        		try {
        			output.submit(alertTuple);
        			_trace.error("Memory is leaking!");
        		} catch (Exception e) {
        			_trace.error("Exception thrown submitting tuple!", e);
        		}

        		alerted = true;
    		}
        } else {
        	// Reset alert if memory has stopped leaking.
        	if(alerted) {
        		alerted = false;
        	}
        }
    }

	@Override
    public void processPunctuation(StreamingInput<Tuple> stream,
    		Punctuation mark) throws Exception {

    	// Output metrics to InfluxDB.
    	if(batchPoints != null) {
    		influxDB.write(batchPoints);
    	}

        // Output metrics to ElasticSearch.
        if(builder != null) {
	        response = client.prepareIndex("streamsdb", "memorymetrics")
	            	.setSource(builder)
	            	.get();
        }
    }

    @Override
    public synchronized void shutdown() throws Exception {
        OperatorContext context = getOperatorContext();
        Logger.getLogger(this.getClass()).trace("Operator " + context.getName() + " shutting down in PE: " + context.getPE().getPEId() + " in Job: " + context.getPE().getJobId() );

        // Close connection to ElasticSearch server.
        client.close();
        
        super.shutdown();
    }
    
    private int calculateRollingAvg(String metricName, long peId) {
    	// Query for memory metrics within specified rolling interval.
    	SearchResponse response = client.prepareSearch("streamsdb")
	        .setTypes("memorymetrics")
	        .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
	        .setQuery(QueryBuilders.existsQuery(metricName))
	        .setQuery(QueryBuilders.termQuery("peId", peId))
	        .setPostFilter(QueryBuilders.rangeQuery("lastTimeRetrieved")
	        		.gte(df.format(new Date(System.currentTimeMillis() - rollingAverageInterval*1000))))
	        .setSize(1200)
	        .get();
    	    	
    	// Calculate rolling average.
    	double rollingAverage = 0;
    	if(response.getHits().totalHits() > 0) {
	    	List<Double> avgList = new ArrayList<Double>();
		    for (SearchHit hit : response.getHits()) {
		    	if(hit.getSource().get(metricName) != null) {
			    	double element = (double)((Integer)hit.getSource().get(metricName)).intValue();
		        	avgList.add(element);
		    	}
	    	}
		    
		    rollingAverage = (int)average(avgList);
    	}
    	
    	return (int)rollingAverage;
	}

	private boolean memoryLeaking(long peId) {
		// Query for memory metrics within the last 10 minutes.
		SearchResponse response = client.prepareSearch("streamsdb")
	        .setTypes("memorymetrics")
	        .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
	        .setQuery(QueryBuilders.existsQuery("nResidentMemoryConsumption"))
	        .setQuery(QueryBuilders.termQuery("peId", peId))
	        .setPostFilter(QueryBuilders.rangeQuery("lastTimeRetrieved")
	        		.gte(df.format(new Date(System.currentTimeMillis() - rollingAverageInterval*1000))))
	        .setSize(1200)
	        .get();
		
		// If, at least, 7.5 minutes have passed, check for memory leak.
		if(response.getHits().hits().length < (rollingAverageInterval/6)) {
			_trace.error("Not enough hits: " + String.valueOf(response.getHits().hits().length));
			return false;
		} else {
			double currentElement = 0;
			double increases = 0;
			double decreases = 0;
			double ratio = 0;
			_trace.error("-------start------");
			for (SearchHit hit : response.getHits()) {
				if(hit.getSource().get("nResidentMemoryConsumption") != null) {
					_trace.error("peid:" + String.valueOf(peId) + ";time:" + hit.getSource().get("lastTimeRetrieved"));
			    	double nextElement = (double)((Integer)hit.getSource().get("nResidentMemoryConsumption")).intValue();
			    	
			    	// If memory consumption is fluctuating, assume no memory leak.
		        	if(currentElement < nextElement) {
		        		increases++;
		        	} else {
		        		decreases++;
		        	}
		        	currentElement = nextElement;
				}
	    	}
			
			ratio = (increases/(increases+decreases))*100;
			
			_trace.error("pe:" + peId +
					";inc:" + String.valueOf(increases) +
    				";dec:" + String.valueOf(decreases) 
    				+ ";ratio:" + String.valueOf(ratio) + "%");
			
			// If memory consumption is increasing 75% of the time, assume memory leak.
			if(ratio > 75.0) {
				_trace.error("Memory leaking!");
				return false;
			} else {
				return false;
			}
		}
	}

	// Calculate rolling average of a list.
	private double average(List<Double> list) {
    	double sum = 0;
    	double size = (double)list.size();
		for(double value : list) {
			_trace.error("value:" + String.valueOf(value));
			sum += value;
		}
		
		if(size == 0) {
			size = 1.0;
		}
		

		_trace.error("size:" + String.valueOf(size));

		_trace.error("avg:" + String.valueOf(sum/size));
		return (sum/size);
	}
    
}
